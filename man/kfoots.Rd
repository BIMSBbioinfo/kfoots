% Generated by roxygen2 (4.1.0): do not edit by hand
% Please edit documentation in R/kfoots.R
\name{kfoots}
\alias{kfoots}
\title{Fit mixture model or a hidden markov model}
\usage{
kfoots(counts, k, framework = c("HMM", "MM"), mix_coeff = NULL,
  trans = NULL, initP = NULL, tol = 1e-04, maxiter = 200,
  nthreads = 1, nbtype = c("dep", "indep", "pois"), init = c("pca",
  "counts", "rnd"), init.nlev = 20, verbose = TRUE,
  seqlens = ncol(counts))
}
\arguments{
\item{counts}{matrix of non-negative integers. Columns represent datapoints and rows
dimensions}

\item{k}{either the desired number of cluster, or a specific initial
value for the models (mixture components or emission probabilities).
See the item \code{models} in the return values to see how the
model parameters should be formatted}

\item{framework}{Switches between a mixture model and a hidden markov model.
The default is a hidden markov model, where the order of the datapoints
matters.}

\item{mix_coeff}{In the \code{MM} mode, initial value for the mixture
coefficients. In the \code{HMM} mode it will be ignored.}

\item{trans}{In the \code{HMM} mode, initial value for the transition
probabilities as a square matrix. The rows are the 'state from' and
the columns are the 'state to', so each rows must sum up to 1.
In the \code{HMM} mode it will be ignored.}

\item{initP}{In the \code{HMM} mode, initial probabilities for each
sequence of observation. They must be formatted as a matrix where
each row is a state and each column is a sequence.}

\item{tol}{Tolerance value used to determine convergence of the EM
algorithm. The algorithm will converge when the absolute difference
in the log-likelihood between two iterations will fall below this value.}

\item{maxiter}{maximum number of iterations in the EM algorithm}

\item{nthreads}{number of threads used. The backward-forward step in the HMM learning
cannot use more threads than the number of sequences.}

\item{nbtype}{type of training for the negative binomial. Accepted types are:
\code{indep}, \code{dep}, \code{pois}. The first type corresponds to standard
maximum likelihood estimates for each parameter of each model, the second one
forces the \code{r} dispersion parameters of the negative multinomials to be the same
for all models, the third one forces \code{r} to be infinity, that is, every model
will be a Poisson distribution.}

\item{init}{Initialization scheme for the models (mixture components or emission
probabilities). The value \code{rnd} results in parameters being chosen randomly,
the values \code{counts, pca} use an initialization algorithm that starts from
\code{init.nlev*nrow(counts)} clusters and reduces them to \code{k} using
hierachical clustering.}

\item{init.nlev}{Tuning parameter for the initialization schemes \code{counts, pca}.}

\item{verbose}{print some output during execution}

\item{seqlens}{Length of each sequence of observations. The number of columns
of the count matrix should equal \code{sum(seqlens)}.}
}
\value{
a list with, among other, the following parameters:
	\item{models}{a list containing the parameters of each model
	(mixture components or emission probabilities). Each element of
	the list describes a negative multinomial distribution.
This is specified in another list with items \code{mu}, \code{r} and \code{ps}. \code{mu} and
		\code{r} correspond to parameters \code{mu} and \code{size} in the R-function \code{\link{dnbinom}}.
		Ps specifies the parameters of the multinomial and they sum up to 1.}
	\item{loglik}{the log-likelihood of the whole dataset.}
	\item{posteriors}{A matrix of size \code{length(models)*ncol(counts)} containing the posterior
			probability that a given datapoint is generated by the given mixture component}
	\item{states}{An integer vector of length \code{ncol(counts)} saying
		which model each column is associated to (using the posterior decoding
		algorithm).}
	\item{converged}{\code{TRUE} if the algorithm converged in the given number of iterations, \code{FALSE} otherwise}
	\item{llhistory}{time series containing the log-likelihood of the
		whole dataset across iterations}
	\item{viterbi}{In HMM mode, the viterbi path an its likelihood as a list.}
}
\description{
Fit mixture model or a hidden markov model
}

