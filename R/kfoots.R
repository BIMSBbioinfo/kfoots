#' Models for multivariate count data
#'
#' The package provides methods for fitting multivariate count data with
#' a mixture model. Each mixture component is a negative multinomial 
#' random variable and an EM algorithm is used to maximize the likelihood.
#'
#' @name kfoots-package
#' @docType package
#' @author Alessandro Mammana \email{mammana@@molgen.mpg.de}
#' @useDynLib kfoots
NULL

#' Wrapper around \code{\link{kfoots_core}}.
#'
#' It implements multiple starting points and outer parallelization. 
#' @param counts see documentation for \code{\link{kfoots_core}}
#' @param models either the desired number of cluster, or a specific initial
#' value for the models. In the second case and if \code{nstart}>1, only the first
#' iteration will use the specified initial parameters.
#' @param nstart number of starting points
#' @param output print some output during execution
#' @param cores number of cores to be used. It is used to run the different
#' initializations of \code{kfoots_core} in parallel, \code{kfoots_core} is not parallelized yet.
#' Setting \code{cores > nstart} is the same as setting \code{cores = nstart}.
#' @param ... see documentation for \code{\link{kfoots_core}}
#' @return see documentation for \code{\link{kfoots_core}}. The followings are additional
#' items added to the list:
#' 	\item{converged_vs_total}{if \code{nstart} > 1, the total number of times the 
#' algorithm converged vs the number of inizialisations.}
#' @export
kfoots <- function(counts, k, nstart=1, verbose=FALSE, cores=1, ...){
	converged <- 0
	bestFoots <- NA
	bestLlik <- -Inf
	if (cores <= 1 || nstart == 1){#no parallelization
		for (i in 1:nstart){
			foots <- kfoots_core(counts, k, verbose=verbose, ...)
			
			if (foots$loglik > bestLlik)
				bestFoots <- foots
				bestLlik <- foots$loglik
			
			if (foots$converged)
				converged <- converged + 1
			
			#if initial parameters were specified,
			#from 2nd iteration on use random initial parameters
			if (is.list(k))
				k <- length(k)
		}
	} else {#parallelize with mclapply
		require(multicore)
		if (is.list(k))
			iargs <- c(list(k), rep(length(k), nstart-1))
		else
			iargs <- rep(k, nstart)
		
		footslist <- mclapply(mc.cores=min(cores, nstart), iargs, function(currk){
			kfoots_core(counts, currk, verbose=verbose, ...)
		})
		
		for (foots in footslist){
			if (foots$loglik > bestLlik)
				bestFoots <- foots
				bestLlik <- foots$loglik
			
			if (foots$converged)
				converged <- converged + 1
		}
		
	}
	
	if (nstart > 1){
		if (verbose)
			cat("The algorithm converged", converged, "times out of", nstart, "\n")
		bestFoots$converged_vs_total = converged/nstart
	}
	
	bestFoots
}

#' Fit mixture model
#'
#' Fit a mixture of negative multinomials to some count data
#' @param counts matrix of non-negative integers. Columns represent datapoints and rows
#' 	dimensions 
#' @param k either the desired number of cluster, or a specific initial
#' 	value for the models. See the item \code{models} in the return values to see how the
#' 	model parameters should be formatted
#' @param mix_coeff initial value for the mixture coefficients. If not provided
#' 	equally likely clusters will be assumed at the first iteration.
#' @param tol error tolerance used when checking whether the parameters have
#' 	changed from one iteration to the next
#' @param maxiter maximum number of iterations in the EM algorithm
#' @param verbose print some output during execution
#' @return a list with the parameters of the fitted model:
#' 	\item{models}{a list containing the parameters of each model.
#' 		These are specified in another list with items \code{mu}, \code{r} and \code{ps}. \code{mu} and
#' 		\code{r} correspond to parameters \code{mu} and \code{size} in the R-function \code{\link{dnbinom}}.
#' 		Ps specifies the parameters of the multinomial and they sum up to 1.}
#' 	\item{mix_coeff}{the mixture coefficients of the mixture model}
#' 	\item{loglik}{the log-likelihood of the whole dataset.}
#'		\item{posteriors}{A matrix of size \code{length(models)*ncol(counts)} containing the posterior
#'			probability that a given datapoint is generated by the given mixture component}
#'		\item{converged}{\code{TRUE} if the algorithm converged in the given number of iterations, \code{FALSE} otherwise}
#'		\item{llhistory}{time series containing the log-likelihood of the
#'			whole dataset across iterations}
#' @export
kfoots_core <- function(counts, k, mix_coeff=NULL, tol = 1e-8, maxiter=100, nthreads=1, verbose=FALSE){
	if (!is.matrix(counts))
		stop("invalid counts variable provided. It must be a matrix")
	#this will ensure efficiency of certain methods.
	#all floating point numbers will be "floored" (not rounded)
	storage.mode(counts) <- "integer"
	
	models <- NULL
	if (!is.numeric(k)){
		if (!is.list(k)){
			stop("Invalid input value for k, provide the desired number of models or a list with their initial parameters")
		}
		models <- k
		k <- length(models)
	}
	
	#rows of the count matrix represent positions of the footprint
	footlen <- nrow(counts)
	#columns of the count matrix represent genomic loci
	nloci <- ncol(counts)
	#precompute some stuff for optimization
	ucs <- mapToUnique(colSumsInt(counts, nthreads))
	mConst <- getMultinomConst(counts, nthreads)
	 
	if (is.null(models)){
		#get initial random models. Need to be kind-of similar to
		#the count matrix, cannot be completely random
		models = rndModels(counts, k, bgr_prior=0.5, ucs=ucs)
	}
	if (is.null(mix_coeff)){
		mix_coeff = rep(1/k, k)
	}
	
	#allocating memory
	posteriors <- matrix(0, nrow=k, ncol=nloci)
	lliks <- matrix(0, nrow=k, ncol=nloci)
	
	loglik <- NA
	converged <- FALSE
	llhistory <- numeric(maxiter)
	for (iter in 1:maxiter){
		lLikMat(lliks=lliks, counts, models, ucs=ucs, mConst=mConst, nthreads=nthreads)
		
		res <- llik2posteriors(posteriors=posteriors, lliks, log(mix_coeff), nthreads=nthreads)
		new_loglik <- res$tot_llik
		
		if (verbose){
			cat("Iteration: ", iter, ", log-likelihood: ", new_loglik, "\n")
		}
		
		new_models <- list()
		for (m in 1:k){
			new_models[[m]] <- fitModel(counts, posteriors[m,], models[[m]]$r, ucs=ucs, nthreads=nthreads)
		}
		if(iter!=1 && new_loglik < loglik && !compare(new_loglik, loglik, tol))
			warning(paste0("decrease in log-likelihood at iteration ",iter))
		
		new_mix_coeff <- rowSums(posteriors)
		new_mix_coeff <- new_mix_coeff / sum(new_mix_coeff)
		
		if (all(compare(mix_coeff, new_mix_coeff,tol))){
			if (all(sapply(c(1:k), function(m) compareModels(models[[m]], new_models[[m]], tol)))){
				converged <- TRUE
			}
		}
		
		mix_coeff <- new_mix_coeff
		models <- new_models
		loglik <- new_loglik
		llhistory[iter] <- loglik
		if (converged){
			break
		}
	}
	if (!converged)
		warning(paste0("The algorithm did not converge after ", maxiter, " iterations (try to increase parameter maxiter)"))
	
	for (model in models)
		names(model$ps) <- rownames(counts)
	
	list(models=models, mix_coeff=mix_coeff, loglik = loglik,
	posteriors=posteriors, converged = converged, llhistory=llhistory[1:iter])
}


#to be parallelized, or avoid iteration through the whole matrix
rndModels <- function(counts, k, bgr_prior=0.5, ucs=NULL){
	seeds <- getUniqueSeeds(counts, k)
	modelsFromSeeds(counts, seeds, bgr_prior=bgr_prior, ucs=ucs)
}

#seeds are columns of the count matrix which are guaranteed to be distinct.
#they are used to initialize the models
modelsFromSeeds <- function(counts, seeds, bgr_prior=0.5, ucs=NULL){
	models = list()
	bgr = rep(1, ncol(counts))
	for (i in seq_along(seeds)){
		posteriors = bgr
		posteriors[seeds[i]] = bgr[seeds[i]] + 1-bgr_prior
		
		models[[i]] = fitModel(counts, posteriors, ucs=ucs) 
	}
	
	models
}

#find k unique seeds as fast as possible given that "counts" could be huge.
#It sorts increasing subsets of the counts matrix until it finds a sufficient
#number of unique elements
getUniqueSeeds <- function(counts, k){
	#shuffle the matrix, dupicated columns are likely to be adjacent
	shuffle <- sample(ncol(counts), ncol(counts))
	if (k==1)
		return(shuffle[1])
	invert_shuffle <- integer(length=ncol(counts))
	invert_shuffle[shuffle] <- 1:ncol(counts)
	counts <- counts[, shuffle]
		
	#increase the size of the sorted matrix by 2 every time
	old_size <- 0
	size <- k
	unique_cols <- c()
	
	while (old_size < ncol(counts)){
		#concatenate the old ones with the new ones
		unique_cols <- c(unique_cols, uniqueColumns(counts[,(old_size+1):size]))
		if (length(unique_cols)>1) #check if there are duplicates (merge the two sets)
			unique_cols <- uniqueColumns(counts[,unique_cols])
		
		if (length(unique_cols)>=k){
			return (invert_shuffle[unique_cols[1:k]])
		}
		old_size <- size
		size <- min(2*size, ncol(counts))
	}
	stop(paste("At least",k,"distinct columns are needed, found",length(unique_cols)))
}

#return indices of first occurrences of all unique columns in the matrix
#(it's a bit faster than duplicated(counts, MARGIN=2) but it does the same)
uniqueColumns <- function(counts){
	#lexicographic sorting of the columns (loci)
	o <- do.call(order, as.list(as.data.frame(t(counts))))
	io <- numeric(length=length(o)); io[o] <- 1:length(o)
	counts <- counts[,o]
	#identify runs of identical columns
	d <- .colSums(abs(counts - counts[, c(1, 1:(ncol(counts)-1))]), nrow(counts), ncol(counts))>0
	d[1] <- TRUE
	#get leading indexes positions
	io[(1:ncol(counts))[d]]
}


#counts columns are loci and rows are positions of the footprint
#in EM it is important to set old_r as the previous value for the same model,
#this ensures that the fitted value is better than the old one
#and it is necessary for the likelihood to increase
#ucs is the result of map2unique(colSums(counts)) and it is used to speed up computation
fitModel <- function(counts, posteriors=NULL, old_r=NULL, maxit=100, ucs=NULL, nthreads=1){
	if (is.null(posteriors))
		posteriors <- rep(1.0, ncol(counts))
	
	#fitting the multinomial
	ps <- fitMultinom(counts, posteriors, nthreads=nthreads)
	
	#fitting the negative binomial
	#if ucs is provided, no need to compute colSums
	if (is.null(ucs))
		ucs <- mapToUnique(colSumsInt(counts, nthreads))
		
	res <- fitNB(ucs, posteriors=posteriors, old_r=old_r, maxit=maxit)
	res$ps <- ps
	
	res
}

#' Fit a negative binomial distribution
#'
#' Maximum Likelihood Estimate for the parameters of a negative binomial distribution
#' generating a specified vector of counts. The MLE for the negative binomial
#' should not be used with a small number of datapoints, it is known to be
#' biased.
#' @param counts a vector of counts. If a list is given, then it is assumed 
#' 	to be the result of the function \code{mapToUnique(counts)}
#' @param posteriors a vector specifying a weight for each count. The maximized
#' 	function is: \eqn{\sum_{i=1}{L}{posteriors[i]\log(Prob\{counts[i]\}}}. If
#' 	not specified, equal weights will be assumed
#' @param old_r an initial value for the size parameter of the negative binomial.
#' 	If not specified the methods of moments will be used for an initial guess.
#' @param maxit maximum number of iterations of the gradient descent to find the
#' 	best size parameter
#' @param nthreads number of threads. Too many threads might worsen the 
#' 	performance
#' @return A list with the parameters of the negative binomial.
#' 	\item{mu}{the mu parameter}
#'		\item{r}{the size parameter}
#' @export
fitNB <- function(counts, posteriors=NULL, old_r=NULL, maxit=100, nthreads=1){
	#transforming the counts into unique counts
	if (!is.list(counts)){
		ucs <- mapToUnique(counts)
	} else {
		ucs <- counts
	}

	if (is.null(posteriors))
		posteriors <- rep(1.0, length(ucs$map))
	
	counts <- ucs$values
	posteriors <- sumAt(posteriors, ucs$map, length(counts), zeroIdx=TRUE)
	
	
	spost <- sum(posteriors)
	mu <- sum(posteriors*counts)/spost
	if (is.null(old_r)){
		#figure out a reasonable estimate for r.
		#match the second moment
		v <- sum(posteriors*counts^2)/spost - mu^2
		old_r <- mu^2/(v - mu)
	}
	
	f <- function(logr) {
		#-sum(nbinom_logLik(counts, mu, exp(logr))*posteriors, na.rm=T)
		-optimFun(counts, mu, exp(logr), posteriors, nthreads)
	}
	o <- optim(log(old_r), f, lower=-30, upper=10, method="L-BFGS-B", control=list(maxit=maxit))
	r <- exp(o$par)
	list(mu=mu, r=r)
}


compareModels <- function(m1, m2, tol){
	v1 <- c(m1$ps, m1$mu, m1$r)
	v2 <- c(m2$ps, m2$mu, m2$r)
	all(compare(v1, v2, tol))
}

compare <- function(c1, c2, tol){
	abs(c1-c2) <= tol*abs(c1+c2)
}
generateCol <- function(model){
	rmultinom(1, rnbinom(1, mu=model$mu, size=model$r), prob=model$ps)
}

generateData <- function(n, models, mix_coeff){
	mat <- matrix(0L, ncol=n, nrow=length(models[[1]]$ps))
	for (i in 1:n){
		model <- models[[sample(length(mix_coeff), 1, prob=mix_coeff)]]
		mat[,i] <- generateCol(model)
	}
	mat
}


generateIndependentData <- function(n, models, mix_coeff){
	mat <- matrix(0L, ncol=n, nrow=length(models[[1]]$ps))
	comp <- sample(length(mix_coeff), n, prob=mix_coeff, replace=T)
	for (i in seq_along(mix_coeff)){
		model = models[[i]]
		n = sum(comp==i)
		for (j in seq_along(model$ps)){
			mat[j,comp==i] <- as.integer(rnbinom(n, mu=model$mu*model$ps[j], size=model$r))
		}
	}
	mat
}

exampleData <- function(n=10000, indip=FALSE){
	m1 = list(mu=40, r=0.4, ps=c(1,8,5,8,5,6,5,4,3,2,1))
	m2 = list(mu=20, r=2, ps=c(1,1,1,1,1,3,4,5,6,5,4))
	m1$ps = m1$ps/sum(m1$ps)
	m2$ps = m2$ps/sum(m2$ps)
	p1 = 0.3
	p2 = 0.7
	
	if (indip)
		generateIndependentData(n, list(m1, m2), c(p1,p2))
	else
		generateData(n, list(m1, m2), c(p1,p2))
}



debugLikelihoodDecrease <- function(){
	if (TRUE){
			old_mll = sum(getLlik(counts, models[[m]])*posteriors[m,], na.rm=T)
			new_mll = sum(getLlik(counts, new_models[[m]])*posteriors[m,], na.rm=T)
			if (is.na(new_mll)){
				print("got na from model: ")
				print(new_models[[m]])
				print("mixing coefficients: ")
				print(mix_coeff)
			}
			if(
				iter > 1 && 
				old_mll > new_mll &&
				!compare(old_mll, new_mll, tol)
				
			){
				
				print(paste0("fitted model worse than the old one at iteration ", iter))
				
				if (
					sum(nbinom_logLik(colSums(counts), models[[m]]$mu, models[[m]]$r)*posteriors[m,], na.rm=T) >
					sum(nbinom_logLik(colSums(counts), new_models[[m]]$mu, new_models[[m]]$r)*posteriors[m,], na.rm=T)
				){
					print("fitted parameters for the negative binomial are worse")
					print(sum(nbinom_logLik(colSums(counts), models[[m]]$mu, models[[m]]$r)*posteriors[m,]))
					print(sum(nbinom_logLik(colSums(counts), new_models[[m]]$mu, new_models[[m]]$r)*posteriors[m,]))
				}
				if (
					sum(multinom_logLik(counts, models[[m]]$ps)*posteriors[m,]) >
					sum(multinom_logLik(counts, new_models[[m]]$ps)*posteriors[m,])
				){ print("fitted parameters for the multinomial are worse")}
				
				print("Old model: ")
				print(models[[m]])
				print("New model: ")
				print(new_models[[m]])
				stop()
			}}
}
