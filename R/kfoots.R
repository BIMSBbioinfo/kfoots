#' Models for multivariate count data
#'
#' The package provides methods for fitting multivariate count data with
#' a mixture model. Each mixture component is a negative multinomial 
#' random variable and an EM algorithm is used to maximize the likelihood.
#'
#' @name kfoots-package
#' @docType package
#' @author Alessandro Mammana \email{mammana@@molgen.mpg.de}
#' @useDynLib kfoots
NULL

#' Wrapper around \code{\link{kfoots_core}}.
#'
#' It implements multiple starting points and outer parallelization. 
#' @param counts see documentation for \code{\link{kfoots}}
#' @param models either the desired number of cluster, or a specific initial
#' value for the models. In the second case and if \code{nstart}>1, only the first
#' iteration will use the specified initial parameters.
#' @param nstart number of starting points
#' @param output print some output during execution
#' @param cores number of cores to be used. It is used to run the different
#' initializations of \code{kfoots} in parallel. Note that this option is  
#' compatible with the \code{nthreads} option in \code{kfoots}, resulting in
#' a maximum of \code{nthreads*cores} used cores.
#' Setting \code{cores > nstart} is the same as setting \code{cores = nstart}.
#' @param ... see documentation for \code{\link{kfoots}}
#' @return see documentation for \code{\link{kfoots}}. The followings are additional
#' items added to the list:
#' 	\item{converged_vs_total}{if \code{nstart} > 1, the total number of times the 
#' algorithm converged vs the number of inizialisations.}
#' @export
kfoots_wrapper <- function(counts, k, nstart=1, verbose=FALSE, cores=1, ...){
	converged <- 0
	bestFoots <- NA
	bestLlik <- -Inf
	if (cores <= 1 || nstart == 1){#no parallelization
		for (i in 1:nstart){
			foots <- kfoots(counts, k, verbose=verbose, ...)
			
			if (foots$loglik > bestLlik)
				bestFoots <- foots
				bestLlik <- foots$loglik
			
			if (foots$converged)
				converged <- converged + 1
			
			#if initial parameters were specified,
			#from 2nd iteration on use random initial parameters
			if (is.list(k))
				k <- length(k)
		}
	} else {#parallelize with mclapply
		require(multicore)
		if (is.list(k))
			iargs <- c(list(k), rep(length(k), nstart-1))
		else
			iargs <- rep(k, nstart)
		
		footslist <- mclapply(mc.cores=min(cores, nstart), iargs, function(currk){
			kfoots(counts, currk, verbose=verbose, ...)
		})
		
		for (foots in footslist){
			if (foots$loglik > bestLlik)
				bestFoots <- foots
				bestLlik <- foots$loglik
			
			if (foots$converged)
				converged <- converged + 1
		}
		
	}
	
	if (nstart > 1){
		if (verbose)
			cat("The algorithm converged", converged, "times out of", nstart, "\n")
		bestFoots$converged_vs_total = converged/nstart
	}
	
	bestFoots
}

#' Fit mixture model
#'
#' Fit a mixture of negative multinomials to some count data
#' @param counts matrix of non-negative integers. Columns represent datapoints and rows
#' 	dimensions 
#' @param k either the desired number of cluster, or a specific initial
#' 	value for the models. See the item \code{models} in the return values to see how the
#' 	model parameters should be formatted
#' @param mix_coeff initial value for the mixture coefficients. If not provided
#' 	equally likely clusters will be assumed at the first iteration.
#' @param tol error tolerance used when checking whether the parameters have
#' 	changed from one iteration to the next
#' @param maxiter maximum number of iterations in the EM algorithm
#' @param nbtype type of training for the negative binomial. Accepted types are:
#' 	\code{indep}, \code{dep}, \code{pois}. The first type corresponds to standard
#'.	maximum likelihood estimates for each parameter of each model, the second one
#'.	forces the \code{r} dispersion parameters of the negative multinomials to be the same
#' 	for all models, the third one forces \code{r} to be infinity, that is, every model
#' 	will be a Poisson distribution. Default is \code{indep}.
#' @param verbose print some output during execution
#' @return a list with the parameters of the fitted model:
#' 	\item{models}{a list containing the parameters of each model.
#' 		These are specified in another list with items \code{mu}, \code{r} and \code{ps}. \code{mu} and
#' 		\code{r} correspond to parameters \code{mu} and \code{size} in the R-function \code{\link{dnbinom}}.
#' 		Ps specifies the parameters of the multinomial and they sum up to 1.}
#' 	\item{mix_coeff}{the mixture coefficients of the mixture model}
#' 	\item{loglik}{the log-likelihood of the whole dataset.}
#'		\item{posteriors}{A matrix of size \code{length(models)*ncol(counts)} containing the posterior
#'			probability that a given datapoint is generated by the given mixture component}
#'		\item{converged}{\code{TRUE} if the algorithm converged in the given number of iterations, \code{FALSE} otherwise}
#'		\item{llhistory}{time series containing the log-likelihood of the
#'			whole dataset across iterations}
#' @export
kfoots <- function(counts, k, mix_coeff=NULL, tol = 1e-8, maxiter=100, nthreads=1,
	nbtype=c("indep","dep","pois"), init=c("rnd","totcount","cool","pca"), init.nlev=5, verbose=FALSE){
	if (verbose)
		cat("kfoots with ", nthreads, " threads\n")
	
	if (!is.matrix(counts))
		stop("invalid counts variable provided. It must be a matrix")
	#this will ensure efficiency of certain methods.
	#all floating point numbers will be "floored" (not rounded)
	storage.mode(counts) <- "integer"
	nbtype <- match.arg(nbtype)
	init <- match.arg(init)
	
	models <- NULL
	if (!is.numeric(k)){
		if (!is.list(k)){
			stop("Invalid input value for k, provide the desired number of models or a list with their initial parameters")
		}
		models <- k
		k <- length(models)
	}
	
	#rows of the count matrix represent positions of the footprint
	footlen <- nrow(counts)
	#columns of the count matrix represent genomic loci
	nloci <- ncol(counts)
	#precompute some stuff for optimization
	ucs <- mapToUnique(colSumsInt(counts, nthreads))
	mConst <- getMultinomConst(counts, nthreads)
	
	
	if (is.null(models)){
		#get initial random models. Need to be kind-of similar to
		#the count matrix, cannot be completely random
		if (init=="rnd"){
			models <- rndModels(counts, k, bgr_prior=0.5, ucs=ucs, nbtype=nbtype, nthreads=nthreads)
		} else if (init=="cool" || init=="pca"){
			init <- initCool(counts, k, nlev=init.nlev, nbtype=nbtype, nthreads=nthreads, axes=ifelse(init=="pca","pca","counts"), verbose=verbose)
			models <- init$models
			mix_coeff <- init$mix_coeff
		} else {
			models <- initByTotCount(counts, k, ucs=ucs, nbtype=nbtype, nthreads=nthreads)
		}
	}
	if (is.null(mix_coeff)){
		mix_coeff = rep(1/k, k)
	}

	#allocating memory
	posteriors <- matrix(0, nrow=k, ncol=nloci)
	lliks <- matrix(0, nrow=k, ncol=nloci)
	
	loglik <- NA
	converged <- FALSE
	llhistory <- numeric(maxiter)
	for (iter in 1:maxiter){
		lLikMat(lliks=lliks, counts, models, ucs=ucs, mConst=mConst, nthreads=nthreads)
		
		res <- llik2posteriors(posteriors=posteriors, lliks, mix_coeff, nthreads=nthreads)
		new_loglik <- res$tot_llik
		new_mix_coeff <- res$new_mix_coeff
		
		if (verbose){
			cat("Iteration: ", iter, ", log-likelihood: ", new_loglik, "\n")
		}
		
		new_models <- fitModels(counts, posteriors, models, ucs=ucs, type=nbtype, nthreads=nthreads)
		
		
		if(iter!=1 && new_loglik < loglik && !compare(new_loglik, loglik, tol))
			warning(paste0("decrease in log-likelihood at iteration ",iter))
		
		
		if (all(compare(mix_coeff, new_mix_coeff,tol))){
			if (all(sapply(c(1:k), function(m) compareModels(models[[m]], new_models[[m]], tol)))){
				converged <- TRUE
			}
		}
		
		mix_coeff <- new_mix_coeff
		models <- new_models
		loglik <- new_loglik
		llhistory[iter] <- loglik
		if (converged){
			break
		}
	}
	if (!converged)
		warning(paste0("The algorithm did not converge after ", maxiter, " iterations (try to increase parameter maxiter)"))
	
	for (i in seq_along(models)){
		names(models[[i]]$ps) <- rownames(counts)
	}
	
	#same as: clusters <- apply(posteriors, 2, which.max)
	clusters <- pwhichmax(posteriors, nthreads=nthreads)
	
	list(models=models, mix_coeff=mix_coeff, loglik = loglik,
	posteriors=posteriors, clusters=clusters, converged = converged, llhistory=llhistory[1:iter])
}


#to be parallelized, or avoid iteration through the whole matrix
initByTotCount <- function(counts, k, ucs=NULL, nbtype="indep", nthreads=1){
	cs <- colSumsInt(counts, nthreads)
	#the basic idea is to divide the counts into quantiles and to set each cluster to a different quantile
	#but we want to detect the "zero-inflated" component, if present, and set it to a cluster
	#it is present if the abundance of 'zeros' is higher than the abundance of 'ones'
	#if this is the case, one cluster will consist of only zeros and ones
	tab <- tabFast(cs)#abundance of each count
	if (length(tab) > 2 && tab[1] > tab[2] && (length(cs) - sum(tab[1:2]) > k-1)) {
		p1 <- sum(tab[1:2])/length(cs) #portion of counts corresponding to the 'zero-inflated' mode
	} else {
		p1 <- 1/k
	}
	ps <- c(p1, rep((1-p1)/(k-1), k-1))
	qs <- cumsum(c(0, ps))
	#perturb cs to resolve ties 
	set.seed(17)
	cs <- cs + rnorm(length(cs), sd=0.001)
	#get thresholds
	thresh <- quantile(cs, qs)
	#get cluster assignments
	thresh[length(thresh)] <- thresh[length(thresh)] + 1 #to include rightmost point in the last cluster
	f <- as.integer(cut(cs, thresh, right=F))
	#check that there is at least one datapoint for each cluster
	check <- tabFast(f)
	if (any(check[2:length(check)] == 0)) stop("initialization by total count failed (too few columns? too many models?)")
	
	#get posteriors
	posteriors <- matrix(0, nrow=k, ncol=length(cs))
	posteriors[k*(0:(length(cs)-1)) + f] <- 1
	
	models <- list()
	for (i in 1:k){ models[[i]] <- list(mu=-1, r=-1, ps=numeric(nrow(counts))) }
	
	fitModels(counts, posteriors, models, ucs=ucs, type=nbtype, nthreads=nthreads)
}


#to be parallelized, or avoid iteration through the whole matrix
rndModels <- function(counts, k, bgr_prior=0.5, ucs=NULL, nbtype="indep", nthreads=1){
	seeds <- getUniqueSeeds(counts, k)
	modelsFromSeeds(counts, seeds, bgr_prior=bgr_prior, ucs=ucs, nbtype=nbtype, nthreads=nthreads)
}

#seeds are columns of the count matrix which are guaranteed to be distinct.
#they are used to initialize the models
modelsFromSeeds <- function(counts, seeds, bgr_prior=0.5, ucs=NULL, nbtype="indep", nthreads=1){
	
	posteriors <- matrix(nrow=length(seeds), ncol=ncol(counts), bgr_prior)
	#perturb row i at column seeds[i]
	perturb_pos <- seeds*length(seeds) + 1:length(seeds)
	posteriors[perturb_pos] <- 1 + bgr_prior
	
	#initialize empty models
	models <- list()
	for (i in seq_along(seeds)){ models[[i]] <- list(mu=-1, r=-1, ps=numeric(nrow(counts))) }
	
	fitModels(counts, posteriors, models, ucs=ucs, type=nbtype, nthreads=nthreads)
}

#find k unique seeds as fast as possible given that "counts" could be huge.
#It sorts increasing subsets of the counts matrix until it finds a sufficient
#number of unique elements
getUniqueSeeds <- function(counts, k){
	#shuffle the matrix, dupicated columns are likely to be adjacent
	shuffle <- sample(ncol(counts), ncol(counts))
	if (k==1)
		return(shuffle[1])
		
	#duplicate the size of the sorted matrix every time
	old_size <- 0
	size <- k
	unique_cols <- c()
	
	while (old_size < ncol(counts)){
		#concatenate the old ones with the new ones
		newcols <- shuffle[(old_size+1):size]
		new_unique_cols <- newcols[uniqueColumns(counts[,newcols, drop=FALSE])]
		unique_cols <- c(unique_cols, new_unique_cols)
		#check if there are duplicates (merge the two sets)
		if (length(unique_cols)>1) 
			unique_cols <- unique_cols[uniqueColumns(counts[,unique_cols, drop=FALSE])]
		
		if (length(unique_cols)>=k)
			return (unique_cols[1:k])
		
		old_size <- size
		size <- min(2*size, ncol(counts))
	}
	stop(paste("At least",k,"distinct columns are needed, found",length(unique_cols)))
}

#return indices of first occurrences of all unique columns in the matrix
#(it's 15 times faster than duplicated(counts, MARGIN=2) but it does the same)
uniqueColumns <- function(counts){
	nc <- ncol(counts)
	nr <- nrow(counts)
	#lexicographic sorting of the columns (loci)
	o <- orderColumns(counts)
	io <- (1:length(o))[o]
	counts <- counts[,o, drop=FALSE]
	#identify runs of identical columns
	d <- .colSums(abs(counts - counts[, c(1, 1:(nc-1)), drop=FALSE]), nr, nc)>0
	d[1] <- TRUE
	#get leading indexes positions
	io[(1:nc)[d]]
	
}


#same as before but no fitting is done for the ps variables
fitNoiseModel <- function(counts, posteriors=NULL, old_r=NULL, maxit=100, ucs=NULL, nthreads=1){
	if (is.null(posteriors))
		posteriors <- rep(1.0, ncol(counts))
	
	#noise model, ps are uniform
	nr <- nrow(counts)
	ps <- rep(1/nr, nr)
	
	#fitting the negative binomial
	#if ucs is provided, no need to compute colSums
	if (is.null(ucs))
		ucs <- mapToUnique(colSumsInt(counts, nthreads))
		
	res <- fitNB(ucs, posteriors=posteriors, old_r=old_r, maxit=maxit)
	res$ps <- ps
	res$tag <- "noise"
	
	res
}

#' Fit a negative binomial distribution
#'
#' Maximum Likelihood Estimate for the parameters of a negative binomial distribution
#' generating a specified vector of counts. The MLE for the negative binomial
#' should not be used with a small number of datapoints, it is known to be
#' biased. Internally, this function is using the brent method to find the
#' optimal dispersion parameter.
#' @param counts a vector of counts. If a list is given, then it is assumed 
#' 	to be the result of the function \code{mapToUnique(counts)}
#' @param posteriors a vector specifying a weight for each count. The maximized
#' 	function is: \eqn{\sum_{i=1}{L}{posteriors[i]\log(Prob\{counts[i]\}}}. If
#' 	not specified, equal weights will be assumed
#' @param old_r an initial value for the size parameter of the negative binomial.
#' 	If not specified the methods of moments will be used for an initial guess.
#' @param nthreads number of threads. Too many threads might worsen the 
#' 	performance
#' @return A list with the parameters of the negative binomial.
#' 	\item{mu}{the mu parameter}
#'		\item{r}{the size parameter}
#' @export
fitNB <- function(counts, posteriors=NULL, old_r=NULL, nthreads=1){
	#transforming the counts into unique counts
	if (!is.list(counts)){
		ucs <- mapToUnique(counts)
	} else {
		ucs <- counts
	}

	if (is.null(posteriors))
		posteriors <- rep(1.0, length(ucs$map))
	
	counts <- ucs$values
	posteriors <- sumAt(posteriors, ucs$map, length(counts), zeroIdx=TRUE)
	
		
	if (is.null(old_r)){
		#the algorithm will figure out a reasonable estimate for r.
		#(match the second moment)
		old_r <- -1
		
	}
	
	fitNB_inner(counts, posteriors, old_r)
	
}


compareModels <- function(m1, m2, tol){
	v1 <- c(m1$ps, m1$mu, m1$r)
	v2 <- c(m2$ps, m2$mu, m2$r)
	all(compare(v1, v2, tol))
}

compare <- function(c1, c2, tol){
	if (length(c1)!=length(c2)) stop("cannot compare vectors of different length")
	(abs(c1-c2) <= tol*abs(c1+c2)) | (is.infinite(c1) & is.infinite(c2))
}

generateCol <- function(model){
	rmultinom(1, rnbinom(1, mu=model$mu, size=model$r), prob=model$ps)
}

generateData <- function(n, models, mix_coeff){
	mat <- matrix(0L, ncol=n, nrow=length(models[[1]]$ps))
	for (i in 1:n){
		model <- models[[sample(length(mix_coeff), 1, prob=mix_coeff)]]
		mat[,i] <- generateCol(model)
	}
	mat
}


generateIndependentData <- function(n, models, mix_coeff){
	mat <- matrix(0L, ncol=n, nrow=length(models[[1]]$ps))
	comp <- sample(length(mix_coeff), n, prob=mix_coeff, replace=T)
	for (i in seq_along(mix_coeff)){
		model = models[[i]]
		n = sum(comp==i)
		for (j in seq_along(model$ps)){
			mat[j,comp==i] <- as.integer(rnbinom(n, mu=model$mu*model$ps[j], size=model$r))
		}
	}
	mat
}

exampleData <- function(n=10000, indip=FALSE){
	m1 = list(mu=40, r=0.4, ps=c(1,8,5,8,5,6,5,4,3,2,1))
	m2 = list(mu=20, r=2, ps=c(1,1,1,1,1,3,4,5,6,5,4))
	m1$ps = m1$ps/sum(m1$ps)
	m2$ps = m2$ps/sum(m2$ps)
	p1 = 0.3
	p2 = 0.7
	
	if (indip)
		generateIndependentData(n, list(m1, m2), c(p1,p2))
	else
		generateData(n, list(m1, m2), c(p1,p2))
}



debugLikelihoodDecrease <- function(){
	if (TRUE){
			old_mll = sum(getLlik(counts, models[[m]])*posteriors[m,], na.rm=T)
			new_mll = sum(getLlik(counts, new_models[[m]])*posteriors[m,], na.rm=T)
			if (is.na(new_mll)){
				print("got na from model: ")
				print(new_models[[m]])
				print("mixing coefficients: ")
				print(mix_coeff)
			}
			if(
				iter > 1 && 
				old_mll > new_mll &&
				!compare(old_mll, new_mll, tol)
				
			){
				
				print(paste0("fitted model worse than the old one at iteration ", iter))
				
				if (
					sum(nbinom_logLik(colSums(counts), models[[m]]$mu, models[[m]]$r)*posteriors[m,], na.rm=T) >
					sum(nbinom_logLik(colSums(counts), new_models[[m]]$mu, new_models[[m]]$r)*posteriors[m,], na.rm=T)
				){
					print("fitted parameters for the negative binomial are worse")
					print(sum(nbinom_logLik(colSums(counts), models[[m]]$mu, models[[m]]$r)*posteriors[m,]))
					print(sum(nbinom_logLik(colSums(counts), new_models[[m]]$mu, new_models[[m]]$r)*posteriors[m,]))
				}
				if (
					sum(multinom_logLik(counts, models[[m]]$ps)*posteriors[m,]) >
					sum(multinom_logLik(counts, new_models[[m]]$ps)*posteriors[m,])
				){ print("fitted parameters for the multinomial are worse")}
				
				print("Old model: ")
				print(models[[m]])
				print("New model: ")
				print(new_models[[m]])
				stop()
			}}
}

#if multinom_const is NA, the function returns the log-likelihood
#minus a constant term that does not depend on the parameters.
#equivalent to
#dmultinom(counts[,i], prob=ps, log=T), if multinom_const is not NA, otherwise
#dmultinom(counts[,i], prob=ps, log=T) - lfactorial(sum(counts[,i]) + sum(lfactorial(counts[,i]))
multinom_logLik <- function(counts, ps, multinom_const=getMultinomConst(counts)){
	nloci <- ncol(counts)
	footlen <- nrow(counts)
	res <- .colSums(counts*log(ps), footlen, nloci, na.rm=T)

	if (!is.na(multinom_const)[1]){
		res + multinom_const
	} else {
		res
	}
}

#if multinom_const is NA, the function returns the log-likelihood
#minus a constant term that does not depend on the parameters.
getLlik <- function(counts, model, ucs=NA, multinom_const=getMultinomConst(counts)){
	res <- multinom_logLik(counts, model$ps, multinom_const=multinom_const)
	if (!(is.na(ucs)[1])){
		res + nbinom_logLik(ucs$values, model$mu, model$r)[ucs$map+1]
	} else {
		res + nbinom_logLik(.colSums(counts, nrow(counts), ncol(counts)), model$mu, model$r)
	}
}

nbinom_logLik <- function(cs, mu, r){
	dnbinom(cs, mu=mu, size=r, log=T)
}

lLik <- function(counts, model, ucs=NULL, multinom_const=NULL, nthreads=1){
	ans <- numeric(ncol(counts))
	if (is.null(ucs)) ucs <- mapToUnique(colSums(counts))
	if (is.null(multinom_const)) multinom_const <- getMultinomConst(counts)
	lLikMat(counts, list(model), ucs, multinom_const, ans, nthreads)
	
	ans
}


fitModels_slow <- function(counts, posteriors, models, ucs, type="full"){
	psmat <- counts %*% t(posteriors)
	psmat <- apply(psmat, 2, function(v) v/sum(v))
	
	
}
